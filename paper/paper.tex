\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

% Graphics path
\graphicspath{{../outputs/figures/}}

% Title
\title{Diabetes Prediction Using Machine Learning: \\
A Comparative Analysis of Classical and Deep Learning Methods}

\author{
Shresht Bhowmick \and Mouad Tiahi \and Xiaole Su \and Colin Johnson \\
\\
\textit{MATH 7243: Machine Learning Theory 1} \\
\textit{Fall 2025}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
\begin{abstract}
%==============================================================================
We present a comparative analysis of machine learning methods for diabetes prediction using the Kaggle Playground Series S5E12 synthetic medical dataset. We evaluate four models: Logistic Regression with ElasticNet regularization, Random Forest, LightGBM, and a Multi-Layer Perceptron neural network. Our experiments on 700,000 training samples with 24 features demonstrate that gradient boosting methods (LightGBM) achieve the best ROC-AUC of 0.724, outperforming deep learning approaches. We find that all models converge to approximately 65\% accuracy, suggesting inherent limitations in the feature set. Our best model achieves 0.696 ROC-AUC on the Kaggle public leaderboard, within 0.5\% of top submissions. We discuss the precision-recall tradeoffs across models and provide insights into why tree-based methods outperform neural networks on tabular medical data.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

Diabetes mellitus is a chronic metabolic disorder affecting 38.4 million Americans (11.6\% of the U.S. population), with an additional 8.7 million adults remaining undiagnosed \cite{cdc2023}. Globally, 589 million adults live with diabetes, a prevalence that has doubled since 1990. Late diagnosis significantly increases the risk of severe complications including heart disease, kidney failure, nerve damage, and blindness. Early detection enables preventive interventions and effective disease management, reducing hospitalizations and long-term healthcare costs.

Machine learning offers a promising approach to diabetes prediction by capturing complex, nonlinear patterns in health metrics that may elude traditional diagnostic criteria. ML models can identify high-risk individuals from routine health data, potentially reducing diagnosis gaps and enabling targeted screening programs.

In this work, we investigate the effectiveness of various machine learning approaches on the Kaggle Playground Series S5E12 dataset, a synthetic medical dataset generated using CTGAN (Conditional Tabular GAN). We compare four models: (1) \textbf{Logistic Regression with ElasticNet regularization}, a linear baseline with combined L1/L2 penalties; (2) \textbf{Random Forest}, an ensemble of decision trees using bagging and feature subsampling; (3) \textbf{LightGBM}, state-of-the-art gradient boosting with leaf-wise growth; and (4) \textbf{Neural Network (MLP)}, a multi-layer perceptron with batch normalization and dropout.

Our contributions include: (1) a systematic comparison of these methods on a large-scale diabetes prediction task, (2) analysis of precision-recall tradeoffs relevant to medical screening, and (3) insights into why gradient boosting outperforms deep learning on tabular medical data.

%==============================================================================
\section{Related Work}
%==============================================================================

Machine learning for diabetes prediction has been extensively studied. Zou and Hastie \cite{zou2005elasticnet} introduced ElasticNet regularization, combining the sparsity of L1 (Lasso) with the grouping effect of L2 (Ridge) penalties. Breiman \cite{breiman2001random} proposed Random Forests, demonstrating the power of ensemble methods for classification tasks. More recently, Ke et al. \cite{ke2017lightgbm} introduced LightGBM, achieving state-of-the-art performance on tabular data through histogram-based gradient boosting with leaf-wise tree growth.

For neural networks, foundational work by Rumelhart et al. \cite{rumelhart1986backprop} on backpropagation enabled deep learning, while Ioffe and Szegedy \cite{ioffe2015batchnorm} and Srivastava et al. \cite{srivastava2014dropout} introduced batch normalization and dropout for improved training stability and generalization.

Recent work on diabetes prediction includes supervised ML ensembles for Type 2 diabetes \cite{diabetes_ensemble2019} and explainable AI approaches for transparent predictions \cite{diabetes_xai2025}. However, comparative studies on large-scale synthetic datasets remain limited.

%==============================================================================
\section{Dataset}
%==============================================================================

We use the Kaggle Playground Series S5E12 dataset\footnote{\url{https://www.kaggle.com/competitions/playground-series-s5e12}}, a synthetic medical dataset generated using CTGAN to preserve privacy while maintaining realistic distributions.

\begin{table}[H]
\centering
\caption{Dataset Statistics}
\label{tab:dataset}
\begin{tabular}{ll}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Training samples & 700,000 \\
Test samples & 300,000 \\
Features & 24 \\
Target & Binary (diabetic/non-diabetic) \\
Class distribution & 62\% diabetic, 38\% non-diabetic \\
\bottomrule
\end{tabular}
\end{table}

The 24 features span demographic information (age, gender, ethnicity), anthropometric measurements (BMI, waist-to-hip ratio), vital signs (blood pressure, heart rate), lipid profile (cholesterol, triglycerides), lifestyle factors (smoking, alcohol consumption, physical activity, diet quality), and medical history (hypertension, family history of diabetes).

Notably, the dataset lacks key clinical markers used in standard diabetes diagnosis: HbA1c (glycated hemoglobin), fasting glucose, and insulin levels. This limitation places an inherent ceiling on predictive performance.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{target_distribution.png}
\caption{Target class distribution showing 62\% diabetic vs 38\% non-diabetic samples.}
\label{fig:target_dist}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{correlation_heatmap.png}
\caption{Feature correlation matrix. Most correlations are weak ($|r| < 0.2$), indicating features are relatively independent. Notable exceptions include BMI-waist\_to\_hip\_ratio (0.76) and systolic-diastolic blood pressure (0.23).}
\label{fig:correlation}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{numerical_distributions.png}
\caption{Distribution of numerical features in the dataset.}
\label{fig:distributions}
\end{figure}

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Data Preprocessing}

Our preprocessing pipeline begins with \textbf{encoding} categorical features (gender, ethnicity, smoking status) using LabelEncoder to convert string labels to integers. \textbf{Numerical features} are then standardized using StandardScaler to zero mean and unit variance ($x' = (x - \mu)/\sigma$), ensuring features with different scales contribute equally to model training.

We use an \textbf{80/20 stratified split} for train-validation, maintaining the 62/38 class ratio in both sets. To handle class imbalance, we compute \textbf{balanced class weights} inversely proportional to class frequencies:
\begin{equation}
    w_c = \frac{n}{k \cdot n_c}
\end{equation}
where $n$ is total samples, $k$ is number of classes, and $n_c$ is samples in class $c$.

\subsection{Model 1: Logistic Regression with ElasticNet}

Logistic regression models the probability of diabetes as:
\begin{equation}
    P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}
\end{equation}

We employ ElasticNet regularization, which combines L1 and L2 penalties:
\begin{equation}
    \min_w \frac{1}{n} \sum_{i=1}^{n} \log(1 + e^{-y_i(w^T x_i)}) + \lambda \left( \alpha \|w\|_1 + (1-\alpha) \|w\|_2^2 \right)
\end{equation}

where $\alpha \in [0,1]$ controls the L1/L2 ratio. We set $\alpha = 0.5$ (equal contribution), $C = 1.0$ (inverse regularization strength), and \texttt{class\_weight='balanced'}. The L1 term induces sparsity by driving irrelevant feature weights to exactly zero, while L2 prevents any single feature from dominating and improves numerical stability.

\subsection{Model 2: Random Forest}

Random Forest \cite{breiman2001random} constructs an ensemble of $B$ decision trees, each trained on a bootstrap sample of the data with random feature subsampling at each split:
\begin{equation}
    \hat{y} = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
\end{equation}

For classification, we use majority voting. Our configuration uses 100 trees with \texttt{max\_depth}=20 to prevent overfitting, \texttt{min\_samples\_split}=5, and balanced class weights. The ensemble approach reduces variance by averaging predictions from trees that overfit in different directions.

\subsection{Model 3: LightGBM}

LightGBM \cite{ke2017lightgbm} is a gradient boosting framework with several key innovations. Unlike level-wise growth in traditional GBDT, LightGBM uses \textbf{leaf-wise growth}, splitting the leaf with maximum gain to achieve better accuracy with fewer splits. It employs \textbf{histogram-based binning} to bucket continuous features into discrete bins, reducing computational complexity from $O(n \cdot d)$ to $O(k \cdot d)$ where $k \ll n$. Additionally, \textbf{Gradient-based One-Side Sampling (GOSS)} prioritizes samples with large gradients while randomly sampling those with small gradients.

The boosting objective minimizes:
\begin{equation}
    \mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{t=1}^{T} \Omega(f_t)
\end{equation}
where $l$ is the log loss and $\Omega$ regularizes tree complexity. Our configuration uses 100 estimators, learning rate 0.1, and 31 leaves.

\subsection{Model 4: Neural Network (MLP)}

We implement a Multi-Layer Perceptron in PyTorch \cite{pytorch} with the architecture:

\begin{center}
\texttt{Input(24) $\rightarrow$ Dense(256) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Dropout(0.3)} \\
\texttt{$\rightarrow$ Dense(128) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Dropout(0.3)} \\
\texttt{$\rightarrow$ Dense(64) $\rightarrow$ BN $\rightarrow$ ReLU $\rightarrow$ Dropout(0.3)} \\
\texttt{$\rightarrow$ Dense(1) $\rightarrow$ Sigmoid}
\end{center}

The network uses \textbf{Batch Normalization} \cite{ioffe2015batchnorm} to normalize layer inputs, accelerating training and providing regularization. \textbf{Dropout} \cite{srivastava2014dropout} randomly zeros 30\% of neurons during training to prevent co-adaptation. Training uses AdamW optimizer with BCEWithLogitsLoss for 40 epochs (batch size 1024), with early stopping at patience 10.

\subsection{Evaluation Metrics}

We evaluate models using multiple metrics: \textbf{Accuracy} = $(TP + TN)/(TP + TN + FP + FN)$; \textbf{Precision} = $TP/(TP + FP)$, measuring how many predicted positives are correct; \textbf{Recall} = $TP/(TP + FN)$, measuring how many actual positives are detected; \textbf{F1 Score} = $2 \cdot \text{Precision} \cdot \text{Recall}/(\text{Precision} + \text{Recall})$; and \textbf{ROC-AUC}, the area under the Receiver Operating Characteristic curve.

ROC-AUC is particularly important for imbalanced datasets as it evaluates ranking quality independent of threshold selection.

%==============================================================================
\section{Experiments and Results}
%==============================================================================

\subsection{Main Results}

Table \ref{tab:results} summarizes performance across all models on the validation set.

\begin{table}[H]
\centering
\caption{Model Performance Comparison}
\label{tab:results}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{ROC-AUC} \\
\midrule
Logistic Regression & 66.5\% & 68.3\% & \textbf{86.4\%} & \textbf{0.763} & 0.695 \\
Random Forest & 65.1\% & 73.6\% & 68.5\% & 0.710 & 0.701 \\
LightGBM & 65.3\% & \textbf{77.4\%} & 62.4\% & 0.691 & \textbf{0.724} \\
Neural Network & 62.4\% & 76.1\% & 57.7\% & 0.656 & 0.696 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{LightGBM achieves the best ROC-AUC} (0.724), confirming that gradient boosting methods are state-of-the-art for tabular data. However, \textbf{Logistic Regression wins on F1} (0.763) due to its high recall (86.4\%), catching most diabetic cases at the cost of more false positives. The \textbf{Neural Network underperforms} all other methods, achieving the lowest accuracy (62.4\%) and F1 (0.656). Notably, \textbf{all models converge to approximately 65\% accuracy}, suggesting the feature set imposes a performance ceiling regardless of model complexity.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{metrics_comparison.png}
\caption{Comparison of evaluation metrics across all four models. LightGBM achieves the highest ROC-AUC while Logistic Regression achieves the highest recall and F1.}
\label{fig:metrics}
\end{figure}

\subsection{Confusion Matrices}

Figures \ref{fig:confusion_lr}--\ref{fig:confusion_nn} show the confusion matrices for each model, revealing different precision-recall tradeoffs.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{logistic_regression_(elasticnet)_confusion.png}
    \caption{Logistic Regression: High recall (catches 56,566 diabetics) but many false positives (26,314).}
    \label{fig:confusion_lr}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{random_forest_confusion.png}
    \caption{Random Forest: More balanced errors with 23,509 TN, 16,045 FP, 20,608 FN, 44,838 TP.}
    \label{fig:confusion_rf}
\end{subfigure}
\caption{Confusion matrices for Logistic Regression and Random Forest.}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{lightgbm_confusion.png}
    \caption{LightGBM: Highest precision with fewer false positives, conservative predictions.}
    \label{fig:confusion_lgb}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{neural_network_(mlp)_confusion.png}
    \caption{Neural Network: Lowest recall, misses many positive cases.}
    \label{fig:confusion_nn}
\end{subfigure}
\caption{Confusion matrices for LightGBM and Neural Network.}
\end{figure}

\subsection{ROC and Precision-Recall Curves}

Figure \ref{fig:roc_pr} shows the ROC and Precision-Recall curves for all models. All models significantly outperform random guessing (diagonal). LightGBM consistently achieves the highest true positive rate for any given false positive rate (AUC=0.724). The precision-recall curves confirm LightGBM's superiority with average precision (AP) of 0.8085 versus 0.7888-0.7919 for other models.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{roc_curves_comparison.png}
    \caption{ROC curves showing LightGBM (AUC=0.724) outperforming other models.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{pr_curves_comparison.png}
    \caption{Precision-Recall curves with LightGBM achieving AP=0.8085.}
\end{subfigure}
\caption{ROC and Precision-Recall curves comparing all four models.}
\label{fig:roc_pr}
\end{figure}

\subsection{Feature Importance}

Analysis of feature importance from Random Forest and LightGBM (Figure \ref{fig:feature_importance}) reveals that the most predictive features are physical activity (minutes per week), family history of diabetes, age, BMI, triglycerides, and LDL cholesterol. These align with known diabetes risk factors, providing face validity for our models.

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{lr_feature_importance.png}
    \caption{Logistic Regression coefficient magnitudes.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth]{rf_feature_importance.png}
    \caption{Random Forest feature importance.}
\end{subfigure}
\vspace{0.5cm}
\begin{subfigure}[b]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{lgb_feature_importance.png}
    \caption{LightGBM feature importance.}
\end{subfigure}
\caption{Feature importance across models. Family history and physical activity consistently rank as top predictors.}
\label{fig:feature_importance}
\end{figure}

\subsection{Neural Network Training Dynamics}

The MLP training curves (Figure \ref{fig:nn_training}) reveal overfitting beginning around epoch 15-20, despite dropout and batch normalization. Early stopping at epoch 25 prevents further degradation but cannot recover lost generalization. The validation loss plateaus while training loss continues to decrease, a classic sign of overfitting.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{nn_training_curves.png}
\caption{Neural network training curves showing train/validation loss and accuracy over epochs. Overfitting begins around epoch 15-20 as validation loss plateaus while training loss continues to decrease.}
\label{fig:nn_training}
\end{figure}

\subsection{Kaggle Leaderboard}

Our best submission using LightGBM achieved \textbf{0.696 ROC-AUC} on the Kaggle public leaderboard (evaluated on 20\% of test data). The top submission scores 0.705, placing us within 1\% of the best result.

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Why Does LightGBM Win?}

Gradient boosting methods excel on tabular data for several reasons. Trees naturally capture nonlinear feature interactions without explicit feature engineering. Tree-based methods are also robust to irrelevant features, being invariant to monotonic transformations and handling mixed feature types well. Most importantly, unlike images (CNNs) or text (RNNs/Transformers), tabular data lacks inherent spatial or sequential structure that neural networks can exploit---gradient boosting doesn't need such structure to perform well.

\subsection{Why Does the Neural Network Struggle?}

Deep learning's success in vision and NLP stems from exploiting spatial and sequential structure. Tabular data lacks this structure---features are unordered and heterogeneous. The MLP must learn feature interactions from scratch, while tree-based methods handle this naturally through recursive partitioning. Recent work on TabNet and FT-Transformer attempts to address this, but gradient boosting remains dominant for most tabular tasks.

\subsection{The 65\% Accuracy Ceiling}

All models achieve approximately 65\% accuracy despite different architectures and training procedures. This convergence suggests several underlying factors: the dataset lacks definitive diabetes markers (HbA1c, fasting glucose), meaning we're predicting from correlated lifestyle factors rather than causal clinical measurements; CTGAN generation may introduce artifacts that don't perfectly capture real medical data distributions; and many non-diabetics share risk factor profiles with diabetics, making perfect class separation fundamentally impossible.

\subsection{Limitations}

Our study has several limitations:
\begin{itemize}
    \item \textbf{Synthetic data}: CTGAN-generated data may not capture real medical distributions, limiting clinical applicability.
    \item \textbf{Missing clinical markers}: HbA1c, fasting glucose, and insulin levels would significantly improve diagnostic accuracy.
    \item \textbf{Binary classification}: Real diabetes exists on a spectrum including prediabetes, Type 1, and Type 2.
    \item \textbf{No interpretability analysis}: SHAP values would help explain individual predictions, critical for clinical adoption.
\end{itemize}

%==============================================================================
\section{Conclusion}
%==============================================================================

We presented a comparative study of machine learning methods for diabetes prediction on the Kaggle Playground Series S5E12 dataset. Our results demonstrate that \textbf{gradient boosting is state-of-the-art} for tabular classification, with LightGBM achieving the best ROC-AUC (0.724). \textbf{Deep learning struggles on tabular data}---the MLP underperformed all classical methods, overfitting early despite regularization. Perhaps most importantly, \textbf{feature quality matters more than model complexity}: all models hit a $\sim$65\% accuracy ceiling, suggesting better features (clinical markers) would improve results more than fancier models. Finally, \textbf{class imbalance requires careful handling} through balanced class weights and stratified splits.

Our LightGBM model achieved 0.696 ROC-AUC on the Kaggle public leaderboard, within 0.5\% of top submissions. Future work includes threshold optimization for different clinical contexts, SHAP analysis for interpretability, and exploration of specialized tabular deep learning architectures.

\subsection*{Code Availability}

All code is available at: \url{https://github.com/tetraslam/mltheory-project}

%==============================================================================
% References - Numbered style
%==============================================================================
\begin{thebibliography}{99}

\bibitem{breiman2001random}
L. Breiman, ``Random Forests,'' \textit{Machine Learning}, vol. 45, no. 1, pp. 5--32, 2001.

\bibitem{cdc2023}
Centers for Disease Control and Prevention, ``National Diabetes Statistics Report,'' 2023. [Online]. Available: \url{https://www.cdc.gov/diabetes/data/statistics-report}

\bibitem{ioffe2015batchnorm}
S. Ioffe and C. Szegedy, ``Batch Normalization: Accelerating Deep Network Training,'' in \textit{Proc. ICML}, 2015.

\bibitem{ke2017lightgbm}
G. Ke \textit{et al.}, ``LightGBM: A Highly Efficient Gradient Boosting Decision Tree,'' in \textit{Proc. NeurIPS}, 2017.

\bibitem{rumelhart1986backprop}
D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ``Learning representations by back-propagating errors,'' \textit{Nature}, vol. 323, pp. 533--536, 1986.

\bibitem{srivastava2014dropout}
N. Srivastava \textit{et al.}, ``Dropout: A Simple Way to Prevent Neural Networks from Overfitting,'' \textit{JMLR}, vol. 15, no. 1, pp. 1929--1958, 2014.

\bibitem{zou2005elasticnet}
H. Zou and T. Hastie, ``Regularization and variable selection via the elastic net,'' \textit{J. R. Stat. Soc. B}, vol. 67, no. 2, pp. 301--320, 2005.

\bibitem{diabetes_ensemble2019}
``Supervised ML Ensemble for Type 2 Diabetes Prediction,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1910.09356}

\bibitem{diabetes_xai2025}
``Transparent Diabetes Prediction with Explainable AI,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2501.18071}

\bibitem{sklearn}
F. Pedregosa \textit{et al.}, ``Scikit-learn: Machine Learning in Python,'' \textit{JMLR}, vol. 12, pp. 2825--2830, 2011.

\bibitem{pytorch}
A. Paszke \textit{et al.}, ``PyTorch: An Imperative Style, High-Performance Deep Learning Library,'' in \textit{Proc. NeurIPS}, 2019.

\end{thebibliography}

\end{document}
